{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "  # Introdução"
      ],
      "metadata": {
        "id": "BVZ6uns1yR3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook foi feito por Gabriel Matz visando o estudo dirigido para a prova de certificação oficial para o Tensorflow. A descrição da prova está no seguinte link:\n",
        "https://www.tensorflow.org/certificate?hl=pt-br.\n",
        "\n",
        "Utilizei como base os seguintes notebooks(além de sites variados e stack overflow para pequenas consultas):\n",
        "\n",
        "1- https://github.com/mrdbourke/tensorflow-deep-learning\n",
        "\n",
        "2-https://github.com/williamcwi/DeepLearning.AI-TensorFlow-Developer-Professional-Certificate"
      ],
      "metadata": {
        "id": "fkoP8fIRybRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O conteúdo deste terceiro notebook engloba a terceira parte de exigências da prova do Tensorflow, que estão descritas abaixo:\n",
        "\n",
        "\"(3) Classificação de Imagens\n",
        "Você precisa compreender como construir modelos de reconhecimento de imagem e detecção de objetos com redes neurais profundas e redes neurais convolucionais usando o TensorFlow 2.x. Você precisa saber como:\n",
        "\n",
        "❏ Definir redes neurais convolucionais com camadas Conv2D e camadas de pooling.\n",
        "\n",
        "❏ Construir e treinar modelos para processar conjuntos de dados de imagem do mundo real.\n",
        "\n",
        "❏ Entender como usar convoluções para aprimorar sua rede neural.\n",
        "\n",
        "❏ Utilizar imagens do mundo real em diferentes formas e tamanhos.\n",
        "\n",
        "❏ Aplicar aumentação de imagem para evitar overfitting.\n",
        "\n",
        "❏ Utilizar o ImageDataGenerator.\n",
        "\n",
        "❏ Compreender como o ImageDataGenerator rotula imagens com base na estrutura do diretório.\""
      ],
      "metadata": {
        "id": "cJ6_E6C6y8eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desenvolvimento"
      ],
      "metadata": {
        "id": "VIx1u53U0Me6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**❏ Definir redes neurais convolucionais com camadas Conv2D e camadas de pooling.**\n"
      ],
      "metadata": {
        "id": "lZgT_iAgyncp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**❏ Entender como usar convoluções para aprimorar sua rede neural.**"
      ],
      "metadata": {
        "id": "-6-6o3y1y7Ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fontes que mais gostei para entender o conteúdo:\n",
        "\n",
        "\n",
        "\n",
        "1.   https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
        "2.   https://poloclub.github.io/cnn-explainer/ - Meu favorito. Possui ferramentas de vizualização\n",
        "\n",
        "A tabela abaixo do Daniel Bourke apresenta as noções básicas:\n",
        "\n",
        "| **Hyperparameter/Layer type** | **What does it do?** | **Typical values** |\n",
        "| ----- | ----- | ----- |\n",
        "| Input image(s) | Target images you'd like to discover patterns in| Whatever you can take a photo (or video) of |\n",
        "| Input layer | Takes in target images and preprocesses them for further layers | `input_shape = [batch_size, image_height, image_width, color_channels]` |\n",
        "| Convolution layer | Extracts/learns the most important features from target images | Multiple, can create with [`tf.keras.layers.ConvXD`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) (X can be multiple values) |\n",
        "| Hidden activation | Adds non-linearity to learned features (non-straight lines) | Usually ReLU ([`tf.keras.activations.relu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)) |\n",
        "| Pooling layer | Reduces the dimensionality of learned image features | Average ([`tf.keras.layers.AvgPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D)) or Max ([`tf.keras.layers.MaxPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)) |\n",
        "| Fully connected layer | Further refines learned features from convolution layers | [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) |\n",
        "| Output layer | Takes learned features and outputs them in shape of target labels | `output_shape = [number_of_classes]` (e.g. 3 for pizza, steak or sushi)|\n",
        "| Output activation | Adds non-linearities to output layer | [`tf.keras.activations.sigmoid`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) (binary classification) or [`tf.keras.activations.softmax`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) |\n"
      ],
      "metadata": {
        "id": "x7JAfFtgzKlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**❏ Construir e treinar modelos para processar conjuntos de dados de imagem do mundo real.**"
      ],
      "metadata": {
        "id": "HcyWBjYCy6IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolverei nesse notebook o problema do Food101(classificação múltipla)."
      ],
      "metadata": {
        "id": "r63ymkPF1s0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L # checa se está usando gpu. Se estiver no collab, confira o ambiente de execução"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89RYq5k1188W",
        "outputId": "790d55a0-445a-4d46-9ce6-66ec09d88b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-9d9bdfc6-536f-6f8b-f61e-56d9b4a99def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVrUVlim2OmV",
        "outputId": "6a06287b-b1c6-49e1-f693-46b058f38a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/dansbecker/food-101"
      ],
      "metadata": {
        "id": "B5y7DDfhZuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "input_directory = '/content'\n",
        "print(os.listdir(input_directory))\n",
        "if \"food-101\" in os.listdir():\n",
        "    print(\"Dataset já foi baixado\")\n",
        "else:\n",
        "    print(\"Downloading ...\")\n",
        "    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "    print(\"Dataset downloaded!\")\n",
        "    print(\"Extracting..\")\n",
        "    !tar xzvf food-101.tar.gz > /dev/null 2>&1\n",
        "    print(\"Extraction done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpddWxIA5PMM",
        "outputId": "07f36fba-8781-4eed-8cd2-28d18d3625f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'sample_data']\n",
            "Downloading ...\n",
            "--2024-03-29 23:51:39--  http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz [following]\n",
            "--2024-03-29 23:51:39--  https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4996278331 (4.7G) [application/x-gzip]\n",
            "Saving to: ‘food-101.tar.gz’\n",
            "\n",
            "food-101.tar.gz     100%[===================>]   4.65G  19.3MB/s    in 4m 30s  \n",
            "\n",
            "2024-03-29 23:56:09 (17.7 MB/s) - ‘food-101.tar.gz’ saved [4996278331/4996278331]\n",
            "\n",
            "Dataset downloaded!\n",
            "Extracting..\n",
            "Extraction done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista os arquivos e diretórios\n",
        "for dirpath, dirnames, filenames in os.walk(\"food-101\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aDqpbqPajv-",
        "outputId": "10174caf-ca1c-4afa-91aa-91f9fa6c4507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 2 images in 'food-101'.\n",
            "There are 101 directories and 0 images in 'food-101/images'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/ceviche'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/foie_gras'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/prime_rib'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/bread_pudding'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/cup_cakes'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/tacos'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/beet_salad'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pulled_pork_sandwich'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/mussels'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/peking_duck'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/oysters'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/apple_pie'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/french_toast'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/carrot_cake'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/edamame'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/macaroni_and_cheese'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/baklava'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/baby_back_ribs'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/chicken_curry'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/shrimp_and_grits'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/chocolate_cake'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/frozen_yogurt'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/bibimbap'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/chicken_wings'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/ramen'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/grilled_salmon'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/fish_and_chips'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/caesar_salad'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pho'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/garlic_bread'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/takoyaki'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/lobster_bisque'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pad_thai'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/ravioli'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/cheesecake'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/lobster_roll_sandwich'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/gyoza'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/spaghetti_bolognese'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/hot_and_sour_soup'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/ice_cream'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/tuna_tartare'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/macarons'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/risotto'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/hamburger'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/eggs_benedict'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/filet_mignon'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/escargots'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/french_fries'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/churros'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/spaghetti_carbonara'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/guacamole'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/deviled_eggs'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/creme_brulee'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/crab_cakes'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/cheese_plate'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/omelette'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/seaweed_salad'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/strawberry_shortcake'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/gnocchi'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/clam_chowder'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/waffles'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/miso_soup'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/panna_cotta'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/hummus'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pancakes'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/breakfast_burrito'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/club_sandwich'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pork_chop'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/sushi'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/chocolate_mousse'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/red_velvet_cake'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/croque_madame'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/nachos'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/samosa'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/poutine'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/beef_tartare'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/cannoli'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/sashimi'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/huevos_rancheros'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/spring_rolls'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/chicken_quesadilla'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/beignets'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/bruschetta'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/hot_dog'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/donuts'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/caprese_salad'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/falafel'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/beef_carpaccio'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/onion_rings'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/tiramisu'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/french_onion_soup'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/steak'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/greek_salad'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/grilled_cheese_sandwich'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/fried_calamari'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/fried_rice'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/pizza'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/scallops'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/dumplings'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/lasagna'.\n",
            "There are 0 directories and 1000 images in 'food-101/images/paella'.\n",
            "There are 0 directories and 6 images in 'food-101/meta'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré-processamento\n",
        "\n",
        "\n",
        "1.   Separar em teste e treino\n",
        "2.   Usar o ImageDataGenerator para ajustar em tensores\n",
        "\n",
        "Opcional: tf.keras.utils.image_dataset_from_directory. Segundo o próprio Tensorflow ImageDataGenerator será deprecado.(feito nos extras)\n",
        "\n"
      ],
      "metadata": {
        "id": "vwM6DhbxcUTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path('/content/food-101/images/')\n",
        "category_files = {}\n",
        "for category_dir in data_dir.iterdir():\n",
        "    if category_dir.is_dir():\n",
        "        category_name = category_dir.name\n",
        "        category_files[category_name] = list(category_dir.glob('*'))"
      ],
      "metadata": {
        "id": "opU_gIbZcSyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**❏ Utilizar imagens do mundo real em diferentes formas e tamanhos.**\n",
        "\n"
      ],
      "metadata": {
        "id": "cwbUyNG4y7Y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O conjunto de dados tem forma diferente. Você precisa padronizar."
      ],
      "metadata": {
        "id": "4u4GIpAIjyyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparâmetros\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "metadata": {
        "id": "k6L8LBm5hEng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**❏ Utilizar o ImageDataGenerator.**\n"
      ],
      "metadata": {
        "id": "BY1dV7Dxy8lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**❏ Compreender como o ImageDataGenerator rotula imagens com base na estrutura do diretório.**"
      ],
      "metadata": {
        "id": "11pWx3Pjy8uV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Você pode uar o ImageDataGenerator a partir dos diretórios"
      ],
      "metadata": {
        "id": "GBaImfftmz1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                                shear_range=0.2,\n",
        "                                                                zoom_range=0.2, # métodos de aumento de dados\n",
        "                                                                horizontal_flip=True,\n",
        "                                                                validation_split=0.2)\n",
        "train = train_datagen.flow_from_directory(\n",
        "    '/content/food-101/images',\n",
        "    target_size=(img_height, img_width),\n",
        "    shuffle=True, # IMPORTANTE\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',  # Se quiser one hot é 'categorical'\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val = train_datagen.flow_from_directory(\n",
        "    '/content/food-101/images',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "id": "C7MDy4BGlr6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94eef44-e6dd-4cd8-a512-adfb3af18899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos nossas entradas corretamente formatadas, montamos a rede neural."
      ],
      "metadata": {
        "id": "iTlF4PUSnoQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=101\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(), # passando pra 1 dimensão\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train,\n",
        "  epochs=10,\n",
        "  validation_data= val\n",
        ")"
      ],
      "metadata": {
        "id": "jjgAgVI_n5Ei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "761a31f1-7907-4fae-cb89-e7c6b964ade2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 202/2525 [=>............................] - ETA: 13:48 - loss: 4.6276 - accuracy: 0.0122"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1e3e672389c3>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   metrics=['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**❏ Aplicar aumentação de imagem para evitar overfitting.**"
      ],
      "metadata": {
        "id": "QTevyHjcy8Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O aumento de dados pode ser feito tanto por camadas de aumento como(no extra coloquei um exemplo):\n",
        "\n",
        "https://www.tensorflow.org/tutorials/images/data_augmentation?hl=pt-br\n",
        "\n",
        "Ou usando o próprio ImageDataGenerator, como feito acima com o shear_range, zoom_range e flip."
      ],
      "metadata": {
        "id": "POhVw9hik-Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sem usar o ImageDataGenerator"
      ],
      "metadata": {
        "id": "bFT_k7WwrtfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usando o tf.keras.utils.image_dataset_from_directory**"
      ],
      "metadata": {
        "id": "0dTQrH7ahZsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=1,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkMxFqijhWxN",
        "outputId": "47577b8f-9f7e-41f9-d170-bb94f8b7fbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 101000 files belonging to 101 classes.\n",
            "Using 80800 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=1,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSDS1cyohUld",
        "outputId": "c90732ec-32c1-4145-9e04-450718d34bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 101000 files belonging to 101 classes.\n",
            "Using 20200 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning e prefetching para melhorar o desempenho"
      ],
      "metadata": {
        "id": "UP2e2GfujULd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE) #\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Qx9IDXOIiqNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds, train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNsJPypghlzL",
        "outputId": "0ad51b4c-300d-400d-e0f8-3c29f2f641e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
              " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=101\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"), # o aumento de dados é feito com camadas\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpqN3Bc1ixzd",
        "outputId": "fc937612-f14e-4b5f-efc3-5ddf8153d21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2525/2525 [==============================] - 45s 17ms/step - loss: 4.4033 - accuracy: 0.0390 - val_loss: 4.0978 - val_accuracy: 0.0782\n",
            "Epoch 2/10\n",
            "2525/2525 [==============================] - 34s 14ms/step - loss: 3.9632 - accuracy: 0.1014 - val_loss: 3.8162 - val_accuracy: 0.1248\n",
            "Epoch 3/10\n",
            "2525/2525 [==============================] - 34s 14ms/step - loss: 3.7213 - accuracy: 0.1415 - val_loss: 3.6749 - val_accuracy: 0.1512\n",
            "Epoch 4/10\n",
            "2525/2525 [==============================] - 35s 14ms/step - loss: 3.5690 - accuracy: 0.1655 - val_loss: 3.5726 - val_accuracy: 0.1736\n",
            "Epoch 5/10\n",
            "2525/2525 [==============================] - 35s 14ms/step - loss: 3.4738 - accuracy: 0.1842 - val_loss: 3.5026 - val_accuracy: 0.1816\n",
            "Epoch 6/10\n",
            "2525/2525 [==============================] - 35s 14ms/step - loss: 3.3976 - accuracy: 0.1963 - val_loss: 3.4437 - val_accuracy: 0.1946\n",
            "Epoch 7/10\n",
            "2522/2525 [============================>.] - ETA: 0s - loss: 3.3389 - accuracy: 0.2052"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceba como é muito mais rápido"
      ],
      "metadata": {
        "id": "MOTus4TNvGsW"
      }
    }
  ]
}